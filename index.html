<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SPML: System Prompt Meta Language</title>
    <link href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" rel="stylesheet">
    <script src="https://code.jquery.com/jquery-3.6.0.slim.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.9.3/dist/umd/popper.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>
    <style>
        body {
	    padding: 5% 10% 2% 10%;
        }
        h1 {
  	    padding: 2%;
        }
        .chat-bubble {
            border-radius: 25px;
            padding: 20px;
	    background: #ffdd57;
            display: inline-block;
            max-width: 50%;
        }
        .chat-bubble-response {
	    background: #67c2d0;
            color: white;
        }
        .chat-prompt {
            margin-top: 10px;
        }
        .footer {
            margin-top: 50px;
        }
	.chat-example {
	    background: #e7f0fd;
  	    border: 3px solid #d6e1f3;
	    border-radius: 40px;
	    padding: 1% 2%;
	}
	
	.chat-bubble p:last-child {
	    margin-bottom: 0;
	}
	
	/* Clear floats after the columns */
	.row:after {
	    content: "";
	    display: table;
	    clear: both;
	}
 	.card-header {
  	    font-size: 18px;
	}	
	table {
	    font-size: 18px;
	}
	/* Make sure bubbles don't overlap */
	.chat-bubble + .chat-bubble {
	    margin-top: 10px;
	}
        /* Add additional custom CSS here */
	.dataset-columns {
  		text-align: left;
	}	
	.dataset-gen > img {
	  width: 100%;
	  padding-top: 5%;
	  padding-bottom: 5%;
	}
	.my-4, p {
	  margin-bottom: 1.5rem !important;
	  text-align: justify;
	  font-size: 18px;
	}
	.faq.my-5 {
	  text-align: justify;
	}
	img {
	  width: 100%;
	}
	.citation.my-5 {
  	  text-align: left;
	}
	footer > p {
	  text-align: center;
	}
	a {
	    color: inherit;       /* Use the color of the parent element */
	    text-decoration: none;    /* No underlining */
	    font-weight: normal;  /* No bold text unless the surrounding text is bold */
	    background: none;     /* No background */
	    cursor: default;      /* Change cursor to the default arrow instead of the pointer */
	}
    </style>

</head>
<body>
    <div class="container text-center">
        <h1>SPML Chatbot Prompt Injection Dataset</h1>
        <div class="button-group my-3">
            <a href="https://huggingface.co/datasets/reshabhs/SPML_Chatbot_Prompt_Injection" class="btn btn-primary my-1"><i class="fas fa-database"></i> Dataset</a>
            <button type="button" class="btn btn-secondary my-1" onclick="scrollToCite()"><i class="fas fa-quote-right"></i> Cite</button>
            <a href="#" class="btn btn-info my-1 disabled"><i class="fas fa-code"></i> Code (Coming Soon)</a>
            <a href="https://arxiv.org/abs/2402.11755" class="btn btn-success my-1"><i class="fas fa-file"></i> Arxiv Paper</a>
        </div>
        <p class="my-4">
Introducing the SPML Chatbot Prompt Injection Dataset: a robust collection of system prompts designed to create realistic chatbot interactions, coupled with a diverse array of annotated user prompts that attempt to carry out prompt injection attacks. While other datasets in this domain have centered on less practical chatbot scenarios or have limited themselves to "jailbreaking" – just one aspect of prompt injection – our dataset offers a more comprehensive approach. It not only features realistic chatbot definition and user prompts but also seamlessly integrates with existing prompt injection datasets.
        </p>
        <p class="my-4">
Our primary focus is on the actual content of prompt injection payloads, as opposed to the methodologies used to execute the attacks. We are convinced that honing in on the detection of the payload content will yield a more robust defense strategy than one that merely identifies varied attack techniques. 
        </p>

	<img src=data.png></img>
        <div class="dataset-columns my-5">
            <h3>Dataset Description</h3>
<table>
  <tr>
    <th>#</th>
    <th style="width: 20%;">Field</th>
    <th>Description</th>
  </tr>
  <tr>
    <td>1</td>
    <td>System Prompt</td>
    <td>These are the intended prompts for the chatbot, designed for use in realistic scenarios.</td>
  </tr>
  <tr>
    <td>2</td>
    <td>User Prompt</td>
    <td>This field contains user inputs that query the chatbot with the system prompt described in (1).</td>
  </tr>
  <tr>
    <td>3</td>
    <td>Prompt Injection</td>
    <td>This is set to 1 if the user input provided in (2) attempts to perform a prompt injection attack on the system prompt (1).</td>
  </tr>
  <tr>
    <td>4</td>
    <td>Degree</td>
    <td>This measures the intensity of the injection attack, indicating the extent to which the user prompt violates the chatbot's expected operational parameters.</td>
  </tr>
  <tr>
    <td>5</td>
    <td>Source</td>
    <td>This entry cites the origin of the attack technique used to craft the user prompt.</td>
  </tr>
</table>
        </div>

        <div id="methodology" class="dataset-gen my-5">
            <h3>Dataset Generation Methodology</h3>
       	    <img src=gen.png></img> 
            <p>Our process begins with an initial set of system prompts derived from leaked system prompts from several widely-used chatbots powered by LLMs. We employ GPT-4 to extrapolate from these cases, crafting additional system prompts that emulate the style of the original seeds across diverse subject matters. These prompts are then used to create corresponding valid user input for each generated system prompt.

To facilitate the creation of prompts for prompt injection attacks, we dissect each generated system prompt to identify a set of guiding principles or rules they aim to uphold, such as 'speak courteously'. GPT-4 is then tasked with producing an inverse list that semantically negates each rule; for instance, 'speak courteously' is countered with 'speak rudely'.

From this inverse list, multiple rules are selected at random—the quantity of which dictates the complexity of the attack (degree)—and these are provided to GPT-4 alongside an 'attack seed prompt'. The objective is to craft a user prompt that aligns with the chosen contrarian rules but retains the stylistic nuances of the attack seed prompt. This tailored seed prompt may also integrate various other attack strategies, enhancing the sophistication and realism of the generated scenarios.</p>
            <!-- Define each column -->
        </div>

        <div class="faq my-5">
            <h3>FAQs</h3>
            <div id="accordion">
                <div class="card">
                    <div class="card-header">
                        <a href="#faq1" data-toggle="collapse" data-parent="#accordion">Should I use this dataset to train my prompt injection detection model?</a>
                    </div>
                    <div id="faq1" class="collapse">
                        <div class="card-body">
			It is not advisable to train prompt injection detection models on this dataset. Typically, such models look for patterns in user prompts to detect prompt injections. However, the injection payloads in our dataset are subtle and may not be universally malicious. Training your model on the combinations of system and user prompts from our dataset will not ensure generalization until the model understands how the system prompt can be violated by the user prompt. These models require exposure to a wide range of attack techniques, and since our dataset only includes a limited selection applied to diverse payloads, it is not an ideal training source.
                        </div>
                    </div>
                </div>
		<div class="card">
                    <div class="card-header">
                        <a href="#faq2" data-toggle="collapse" data-parent="#accordion">Why were "jailbreak" datasets not included when jailbreaking is considered a form of prompt injection?</a>
                    </div>
                    <div id="faq2" class="collapse">
                        <div class="card-body">
			For the purpose of this dataset, we only considered sources like TensorTrust and Gandalf that provided precise system prompts. The jailbreak dataset is composed of user prompts designed to create LLM responses that breach ethical guidelines without accompanying system prompts. At the time of development, we lacked a clearly defined system prompt to encapsulate this, hence its exclusion.	
                        </div>
                    </div>
                </div>
		<div class="card">
                    <div class="card-header">
                        <a href="#faq3" data-toggle="collapse" data-parent="#accordion">Why haven't attack prompts based on TensorTrust been released?</a>
                    </div>
                    <div id="faq3" class="collapse">
                        <div class="card-body">
			The TensorTrust dataset is not licensed for distribution, which precludes us from releasing attack prompts derived from it.
                        </div>
                    </div>
                </div>
                <!-- Add more FAQs -->
            </div>
        </div>

        <div id="cite" class="citation my-5">
            <h3>Cite </h3>
            <pre id="bibtexCitation">
@misc{sharma2024spml,
    title={SPML: A DSL for Defending Language Models Against Prompt Attacks},
    author={Reshabh K Sharma and Vinayak Gupta and Dan Grossman},
    year={2024},
    eprint={2402.11755},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
            </pre>
		<button class="btn btn-secondary" onclick="copyToClipboard('#bibtexCitation')">Copy</button>
            
        </div>

    </div>

    <script>
        function copyToClipboard(element) {
            var $temp = $("<input>");
            $("body").append($temp);
            $temp.val($(element).text()).select();
            document.execCommand("copy");
            $temp.remove();
            // Optionally, trigger a tooltip/message indicating text was copied
        }
	function scrollToCite() {
    	   document.getElementById("cite").scrollIntoView({ behavior: 'smooth' });
	}
    </script>
</body>
</html>
